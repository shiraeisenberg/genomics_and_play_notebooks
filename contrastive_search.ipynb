{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy4YjwKYbDypdtKDYMLQnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiraeisenberg/genomics_and_play_notebooks/blob/main/contrastive_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playing with contrastive search"
      ],
      "metadata": {
        "id": "1GzNNVI4kF7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bJmPBz7kCZy",
        "outputId": "23e9e243-e8d4-4ea9-f4ae-49beb9ee5e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.24.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.24.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.24.0) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install \"transformers==4.24.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#greedy search - deterministically selects text continuation with highest likelihood measured by the LM\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n",
        "input_ids = tokenizer('HuggingFace company is', return_tensors='pt').input_ids\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "\n",
        "output = model.generate(input_ids, max_length = 128)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"\"+ 100 * '-')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBltrDVFkaQ2",
        "outputId": "0736d244-f9fa-4b2a-fc65-bd64578f90a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "HuggingFace company is a new company that is trying to make it easier for people to find and share their favorite photos.\n",
            "\n",
            "The company is currently in beta testing and is looking for beta testers to help test the service.\n",
            "\n",
            "The company is looking for people to help test the service and provide feedback.\n",
            "\n",
            "The beta testing is currently open to the public and will run until the end of the month.\n",
            "\n",
            "The beta testing is free and will run until the end of the month.\n",
            "\n",
            "The company is looking for people to help test the service and provide feedback.\n",
            "\n",
            "The beta testing is free and will run\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nucleus sampling -- stochastic method, selects the smallest possible sets of top V words such that sum of probability >= p\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n",
        "input_ids = tokenizer('Deepmind company is', return_tensors='pt').input_ids\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "\n",
        "torch.manual_seed(0.)\n",
        "output = model.generate(input_ids, do_sample=True, max_length=128, top_p=0.95, top_k=0)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"\" + 100 * '-')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1xLGLcplp61",
        "outputId": "5c0b34ec-a797-4e3b-beb8-10c73642e6ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Deepmind company is a fundamental invention.\n",
            "\n",
            "... is \"tree-savvy\": Innovators, wherever they are, include creativity in their arsenal (and a lot of it).\n",
            "\n",
            "... heists algorithmically and systematically.\n",
            "\n",
            "... its efforts are overseen by a charismatic leader, who was thus the first AI research manager.\n",
            "\n",
            "These are issues I'm very well familiar with, but as such (and because the site and related materials are free of charge and accessible all the time) I think others need to come to understand this. Bostrom on Turing Machines\n",
            "\n",
            "Carlos Betancourt is a professor at\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contrastive search -- considers both the prob predicted by the LM to maintain semantic coherence and similarity with respect to prev. context to avoid model degeneration\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_name = 'gpt2-large'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
        "model.eval()\n",
        "\n",
        "# prepare the prefix\n",
        "prefix_text = r'DeepMind Company is'\n",
        "input_ids = tokenizer(prefix_text, return_tensors='pt').input_ids\n",
        "\n",
        "# generate the result with contrastive search\n",
        "output = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=512)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"\" + 100 * '-')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "LgOC4sDZnbQu",
        "outputId": "f9d9b45e-b2a7-4e8c-bf23-48c27069f224"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "DeepMind Company is a leader in artificial intelligence (AI). We have a long history of working with companies such as Google, Facebook, Amazon, and Microsoft to build products that improve people's lives, and today we are excited to announce that DeepMind's AlphaGo program has won the game of Go, becoming the first program to defeat a professional Go player.\n",
            "\n",
            "The victory is a testament to the power of deep learning, and to the incredible work of our research team, which has been at the forefront of AI research for the past five years. AlphaGo is one of the most advanced Go programs ever created, and its performance is an important step towards the goal of human-level AI.\n",
            "\n",
            "\"This is the culmination of a decade of hard work,\" said Andy Ng, co-founder and CTO of DeepMind. \"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that can be used in a wide range of applications and to help people live better lives.\"\n",
            "\n",
            "DeepMind's work on Go began in 2010, when it began to train a neural network to play Go using millions of games played by top Go players around the world. Since then, the team has refined the algorithm, adding more and more layers of reinforcement learning to make it better at recognizing patterns and making decisions based on those patterns. In the past year and a half, the team has made significant progress in the game, winning a record-tying 13 games in a row to move into the top four of the world rankings.\n",
            "\n",
            "\"The game of Go is a complex game in which players have to be very careful not to overextend their territory, and this is something that we have been able to improve over and over again,\" said Dr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. \"We are very proud of our team's work, and we hope that it will inspire others to take the next step in their research and apply the same techniques to other problems.\"\n",
            "\n",
            "In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a number of different games, including poker, Go, and chess. This AI system, called Tarsier, was developed in partnership with Carnegie Mellon University and the University of California, Berkeley, and is being used to teach computer vision and machine learning to identify objects in images and recognize speech in natural language. Tarsier has been trained to play the game of Go and other games on a\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contrastive search -- considers both the prob predicted by the LM to maintain semantic coherence and similarity with respect to prev. context to avoid model degeneration\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_name = 'gpt2-large'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
        "model.eval()\n",
        "\n",
        "# prepare the prefix\n",
        "prefix_text = r'Huggingface Company is'\n",
        "input_ids = tokenizer(prefix_text, return_tensors='pt').input_ids\n",
        "\n",
        "# generate the result with contrastive search\n",
        "output = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=512)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"\" + 100 * '-')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPU1oE8_rWGm",
        "outputId": "00c6df54-65cc-48ce-9137-a920ccc32373"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Huggingface Company is a company that makes a lot of things.\n",
            "\n",
            "I have no idea what they make, but it's pretty cool to see them in the game.\n",
            "\n",
            "This is an image of the company's logo:\n",
            "\n",
            "And here's a close up of the nameplate:\n",
            "\n",
            "The company has a few products that are listed on their website, such as this product:\n",
            "\n",
            "This looks like a lot of fun, right? Well, if you're a fan of Star Wars, you're in luck, because they're offering a limited edition set of Darth Vader figurines for $20. The set comes with a set of six figures, and is limited to 1,000 pieces.\n",
            "\n",
            "If you want to check out more information about the company, you can head over to their website.\n",
            "\n",
            "You can follow me on Twitter, add me on Google+, and pick up a copy of my sci-fi novel, The Last Exodus, and its sequel, The Exiled Earthborn. Both books are now in print and available in ebook and audiobook versions. I also have a Tumblr blog that lets you know when new articles are published on Sequart.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}